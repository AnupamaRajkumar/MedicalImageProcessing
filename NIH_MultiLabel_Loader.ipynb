{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import os.path\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data set and splitting it into training, test and validation data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patient ids: training: 21563, validation: 3079, testing: 6161\n"
     ]
    }
   ],
   "source": [
    "#reference : https://aws.amazon.com/blogs/machine-learning/classifying-high-resolution-chest-x-ray-medical-images-with-amazon-sagemaker/\n",
    "data_dir = \"../../../../../../data/kaggle/nih-chest-xrays/data/\"\n",
    "#splitting thr data set into training, validation and testing data sets\n",
    "#70% training data\n",
    "trainper = 0.7\n",
    "#10% validation data\n",
    "valper = 0.1\n",
    "file_name = data_dir + 'Data_Entry_2017.csv'\n",
    "\n",
    "a = pd.read_csv(file_name)\n",
    "patient_ids = a['Patient ID']\n",
    "uniq_pids = np.unique(patient_ids)\n",
    "np.random.shuffle(uniq_pids)\n",
    "total_ids = len(uniq_pids)\n",
    "\n",
    "trainset = int(trainper*total_ids)\n",
    "valset = trainset+int(valper*total_ids)\n",
    "#remaining data is used as a test set\n",
    "testset = trainset+valset\n",
    "\n",
    "train = uniq_pids[:trainset]\n",
    "val = uniq_pids[trainset+1:valset]\n",
    "test = uniq_pids[valset+1:]\n",
    "print('Number of patient ids: training: %d, validation: %d, testing: %d'%(len(train), len(val), len(test)))\n",
    "\n",
    "traindata = a.loc[a['Patient ID'].isin(train)]\n",
    "valdata = a.loc[a['Patient ID'].isin(val)]\n",
    "testdata = a.loc[a['Patient ID'].isin(test)]\n",
    "\n",
    "traindata.to_csv('traindata.csv', sep=',', header=False, index=False)\n",
    "valdata.to_csv('valdata.csv', sep=',', header=False, index=False)\n",
    "testdata.to_csv('testdata.csv', sep=',', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading training data into the data loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "train_transform = transforms.Compose([transforms.Resize(256),\n",
    "                                        transforms.RandomResizedCrop(224),\n",
    "                                        transforms.RandomHorizontalFlip(), # randomly flip and rotate\n",
    "                                        transforms.RandomRotation(10),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize(255),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a bag of words of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_set(csvfile, outputfile):\n",
    "    disease_list = ['Atelectasis', 'Consolidation', 'Infiltration', 'Pneumothorax', 'Edema', 'Emphysema', \\\n",
    "                   'Fibrosis', 'Effusion', 'Pneumonia', 'Pleural_Thickening', 'Cardiomegaly', 'Nodule', 'Mass', \\\n",
    "                   'Hernia']\n",
    "    alldiseases = {disease:i for i,disease in enumerate(disease_list)}\n",
    "    with open(outputfile, 'w') as fp:\n",
    "        with open(csvfile, 'r') as cfile:\n",
    "            line = csv.reader(cfile, delimiter=',')\n",
    "            index = 0\n",
    "            for element in line:\n",
    "                # the first column is the image filename, while the second\n",
    "                # column has the list of diseases separated by |\n",
    "                diseases = element[1].split('|')\n",
    "                fp.write('%d\\t'%index)\n",
    "                for d in alldiseases:\n",
    "                    if d in diseases:\n",
    "                        fp.write('%d\\t'%1)\n",
    "                    else:\n",
    "                        fp.write('%d\\t'%0)\n",
    "                fp.write('images/%s\\n' % element[0])\n",
    "                index += 1\n",
    "                 \n",
    "gen_set('traindata.csv', 'chestxraytrain.lst')\n",
    "gen_set('valdata.csv', 'chestxrayval.lst')\n",
    "gen_set('testdata.csv', 'chestxraytest.lst')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom dataloader - __to do__\n",
    "The idea is, in __getitem__:\n",
    "A. to read the lst file which contains the labels associated with images\n",
    "B. For each image, check which ailment is '1'\n",
    "C. Append those to labels for that image\n",
    "D. Return image and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XRaysTrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_path, transform=None):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.data_len = len(self.data.index)            # csv data length\n",
    "        \n",
    "        self.image_names = np.array(self.data.iloc[:,0])  # image names\n",
    "        self.heights = np.asarray(self.data.iloc[:,8])    # heights are at 8th column \n",
    "        self.widths =  np.asarray(self.data.iloc[:,7])    # widths are at  7th column\n",
    "        \n",
    "        # Only use 'Peumonia' for single-label classification\n",
    "        # createa a new col\n",
    "        #self.data.loc[:,'Infiltration'] = 0\n",
    "        labels = self.data.loc[:,'Finding Labels'].map(lambda x: x.split('|'))\n",
    "        for i,label in enumerate(labels):\n",
    "            if 'Infiltration' in label:\n",
    "                self.data.loc[i,'Infiltration'] = 1\n",
    "        self.labels = torch.LongTensor(self.data.loc[:,'Infiltration'])     # all labels at  1st column\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Read 1 image name\n",
    "        img_name = self.image_names[index]\n",
    "        \n",
    "        # Read 1 image file\n",
    "        folder_idx_range = 13\n",
    "        for folder_idx in range(folder_idx_range):\n",
    "            path_prefix = \"/home/tu-weihengxia/data/kaggle/nih-chest-xrays/data/images_\"\n",
    "            path_suffix = \"/images/\"\n",
    "            img_folder_path = path_prefix + str(folder_idx).zfill(3) + path_suffix\n",
    "            img_path = os.path.join(img_folder_path, img_name)\n",
    "            if(path.exists(os.path.join(img_folder_path, img_name))):\n",
    "                # print(\"image path: \", img_path)\n",
    "                img_as_img = Image.open(os.path.join(img_folder_path, img_name))\n",
    "                break\n",
    "        \n",
    "        img_as_img = img_as_img.convert(\"RGB\")\n",
    "        # Transform image to tensor\n",
    "        img_as_tensor = self.transform(img_as_img)\n",
    "        \n",
    "        # Read 1 label:\n",
    "\n",
    "        image_label = self.labels[index]\n",
    "        \n",
    "        return img_as_tensor, image_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindataLoader = XRaysTrainDataset('traindata.csv', transform = train_transform)\n",
    "trainLoader = torch.utils.data.DataLoader(traindataLoader, batch_size = 10, num_workers = 5, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/group/donut/Anupama/mlmip-2020-notebooks/pytorch-venv/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 4411, in get_value\n    return libindex.get_value_at(s, key)\n  File \"pandas/_libs/index.pyx\", line 44, in pandas._libs.index.get_value_at\n  File \"pandas/_libs/index.pyx\", line 45, in pandas._libs.index.get_value_at\n  File \"pandas/_libs/util.pxd\", line 98, in pandas._libs.util.get_value_at\n  File \"pandas/_libs/util.pxd\", line 83, in pandas._libs.util.validate_indexer\nTypeError: 'str' object cannot be interpreted as an integer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/group/donut/Anupama/mlmip-2020-notebooks/pytorch-venv/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/group/donut/Anupama/mlmip-2020-notebooks/pytorch-venv/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/group/donut/Anupama/mlmip-2020-notebooks/pytorch-venv/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-48-9f3a40d1d291>\", line 12, in __getitem__\n    img = cv2.imread(row['Image Index'])\n  File \"/group/donut/Anupama/mlmip-2020-notebooks/pytorch-venv/lib/python3.6/site-packages/pandas/core/series.py\", line 871, in __getitem__\n    result = self.index.get_value(self, key)\n  File \"/group/donut/Anupama/mlmip-2020-notebooks/pytorch-venv/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 4419, in get_value\n    raise e1\n  File \"/group/donut/Anupama/mlmip-2020-notebooks/pytorch-venv/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 4405, in get_value\n    return self._engine.get_value(s, k, tz=getattr(series.dtype, \"tz\", None))\n  File \"pandas/_libs/index.pyx\", line 80, in pandas._libs.index.IndexEngine.get_value\n  File \"pandas/_libs/index.pyx\", line 90, in pandas._libs.index.IndexEngine.get_value\n  File \"pandas/_libs/index.pyx\", line 138, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'Image Index'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-ad583c0e0efe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/group/donut/Anupama/mlmip-2020-notebooks/pytorch-venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/group/donut/Anupama/mlmip-2020-notebooks/pytorch-venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/group/donut/Anupama/mlmip-2020-notebooks/pytorch-venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/group/donut/Anupama/mlmip-2020-notebooks/pytorch-venv/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/group/donut/Anupama/mlmip-2020-notebooks/pytorch-venv/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 4411, in get_value\n    return libindex.get_value_at(s, key)\n  File \"pandas/_libs/index.pyx\", line 44, in pandas._libs.index.get_value_at\n  File \"pandas/_libs/index.pyx\", line 45, in pandas._libs.index.get_value_at\n  File \"pandas/_libs/util.pxd\", line 98, in pandas._libs.util.get_value_at\n  File \"pandas/_libs/util.pxd\", line 83, in pandas._libs.util.validate_indexer\nTypeError: 'str' object cannot be interpreted as an integer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/group/donut/Anupama/mlmip-2020-notebooks/pytorch-venv/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/group/donut/Anupama/mlmip-2020-notebooks/pytorch-venv/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/group/donut/Anupama/mlmip-2020-notebooks/pytorch-venv/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-48-9f3a40d1d291>\", line 12, in __getitem__\n    img = cv2.imread(row['Image Index'])\n  File \"/group/donut/Anupama/mlmip-2020-notebooks/pytorch-venv/lib/python3.6/site-packages/pandas/core/series.py\", line 871, in __getitem__\n    result = self.index.get_value(self, key)\n  File \"/group/donut/Anupama/mlmip-2020-notebooks/pytorch-venv/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 4419, in get_value\n    raise e1\n  File \"/group/donut/Anupama/mlmip-2020-notebooks/pytorch-venv/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 4405, in get_value\n    return self._engine.get_value(s, k, tz=getattr(series.dtype, \"tz\", None))\n  File \"pandas/_libs/index.pyx\", line 80, in pandas._libs.index.IndexEngine.get_value\n  File \"pandas/_libs/index.pyx\", line 90, in pandas._libs.index.IndexEngine.get_value\n  File \"pandas/_libs/index.pyx\", line 138, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1619, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1627, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'Image Index'\n"
     ]
    }
   ],
   "source": [
    "#image, label = next(iter(trainLoader))\n",
    "\n",
    "#print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
